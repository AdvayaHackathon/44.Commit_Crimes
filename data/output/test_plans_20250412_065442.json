[
  {
    "title": "Test Plan 1 for gate_cse",
    "description": "Generated test plan",
    "questions": "```json\n[\n  {\n    \"title\": \"Test Plan 1: MapReduce Fundamentals\",\n    \"description\": \"Focuses on the core concepts of MapReduce and its execution flow.\",\n    \"time_estimate\": \"60 minutes\",\n    \"questions\": [\n      {\n        \"topic\": \"MapReduce Architecture\",\n        \"difficulty\": \"easy\",\n        \"question\": \"What is the role of YARN in a MapReduce environment?\",\n        \"answer\": \"YARN (Yet Another Resource Negotiator) manages resources and schedules map and reduce tasks on multiple machines in a Hadoop cluster.\",\n        \"explanation\": \"YARN is the resource manager in Hadoop that takes care of allocating resources to various applications running on the cluster, including MapReduce jobs.\"\n      },\n      {\n        \"topic\": \"MapReduce Execution\",\n        \"difficulty\": \"medium\",\n        \"question\": \"Describe the flow of data in a MapReduce job.\",\n        \"answer\": \"Input -> Map -> Shuffle and Sort -> Reduce -> Output\",\n        \"explanation\": \"Data is read from HDFS, processed by map tasks, shuffled and sorted by key, aggregated by reduce tasks, and finally written back to HDFS.\"\n      },\n      {\n        \"topic\": \"MapReduce Components\",\n        \"difficulty\": \"hard\",\n        \"question\": \"What is the difference between a Combiner and a Reducer?\",\n        \"answer\": \"A Combiner performs local aggregation on the output of Mappers before data is sent across the network for shuffling and sorting.  A Reducer performs the final aggregation after shuffle and sort. Combiners are optional and improve efficiency by reducing data transfer.\",\n        \"explanation\": \"Using a combiner reduces the amount of data transferred between mappers and reducers, improving performance. A reducer performs global aggregation on the data after being grouped by key.\"\n      },\n      {\n        \"topic\": \"MapReduce Fault Tolerance\",\n        \"difficulty\": \"easy\",\n        \"question\": \"How does MapReduce achieve fault tolerance?\",\n        \"answer\": \"By restarting failed tasks on different nodes.\",\n        \"explanation\": \"If a node fails during a MapReduce job, YARN will restart the failed task on a different healthy node.\"\n      },\n      {\n        \"topic\": \"Data Processing Layer\",\n        \"difficulty\": \"medium\",\n        \"question\": \"What is the purpose of the Data Processing Layer in Big Data architectures?\",\n        \"answer\": \"It provides APIs for programs like MapReduce and Spark to process data stored in the Data Storage Layer.\",\n        \"explanation\": \"This layer bridges the gap between data storage and the applications that utilize the data.\"\n      },\n        {\n        \"topic\": \"InputSplit\",\n        \"difficulty\": \"medium\",\n        \"question\": \"What is an InputSplit in MapReduce?\",\n        \"answer\": \"A logical chunk of input data that is processed by a single Mapper.\",\n        \"explanation\": \"InputSplits divide the input data into smaller units for parallel processing by Mappers.\"\n      },\n          {\n        \"topic\": \"RecordReader\",\n        \"difficulty\": \"medium\",\n        \"question\": \"What is the role of a RecordReader?\",\n        \"answer\": \"Converts InputSplits into key-value pairs for the Mapper.\",\n        \"explanation\": \"The RecordReader provides a stream of input records to the Mapper in the form of key-value pairs.\"\n\n      },\n                {\n        \"topic\": \"Composing MapReduce\",\n        \"difficulty\": \"hard\",\n        \"question\": \"Name three types of calculations or algorithms that can be implemented using MapReduce.\",\n        \"answer\": \"Counting, summing, sorting, matrix multiplication, finding distinct values, graph processing\",\n        \"explanation\": \"MapReduce is a versatile framework that can be used to implement a variety of algorithms.\"\n      },\n                    {\n        \"topic\": \"JobTracker and TaskTracker\",\n        \"difficulty\": \"easy\",\n        \"question\": \"In older Hadoop versions (pre-YARN), what were the roles of JobTracker and TaskTracker?\",\n        \"answer\": \"JobTracker was the master node responsible for scheduling and monitoring tasks. TaskTrackers were slave nodes that executed assigned map and reduce tasks.\",\n        \"explanation\": \"These components were central to the MapReduce framework before YARN's introduction.\"\n      },\n                      {\n        \"topic\": \"Big Data Architecture Layers\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Name the layer where tools like Hive and Pig reside in a Big Data architecture.\",\n        \"answer\": \"Application Support Layer.\",\n        \"explanation\": \"This layer provides tools and services that simplify interaction with the underlying data processing framework.\"\n      }\n\n    ]\n  },\n\n\n\n\n{\n    \"title\": \"Test Plan 2: Hive and Pig - Data Processing Tools\",\n    \"description\": \"Covers Hive and Pig, focusing on their features, architectures, and use cases.\",\n    \"time_estimate\": \"50 minutes\",\n    \"questions\": [\n{\n        \"topic\": \"Hive Overview\",\n        \"difficulty\": \"easy\",\n        \"question\": \"What is Hive and what is its purpose?\",\n        \"answer\": \"Hive is a data warehousing tool built on top of Hadoop that provides a SQL-like interface (HiveQL) for querying and managing large datasets.\",\n        \"explanation\": \"Hive allows users familiar with SQL to interact with Hadoop data without writing complex MapReduce jobs.\"\n      },\n      {\n        \"topic\": \"Hive Architecture\",\n        \"difficulty\": \"medium\",\n        \"question\": \"What is the role of the Metastore in Hive?\",\n        \"answer\": \"The Metastore acts as the system catalog, storing metadata about Hive tables, schemas, and partitions.\",\n        \"explanation\": \"The Metastore is crucial for Hive to understand the structure and location of data within HDFS.\"\n\n      },\n       {\n        \"topic\": \"Hive Limitations\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Name one limitation of Hive compared to a traditional RDBMS.\",\n        \"answer\": \"Hive does not support update/delete operations or real-time queries. \",\n        \"explanation\": \"Hive is optimized for batch processing and analytical queries, not transactional operations.\"\n      },\n      {\n        \"topic\": \"Pig Overview\",\n        \"difficulty\": \"easy\",\n        \"question\": \"What is Pig and what are its key features?\",\n        \"answer\": \"Pig is a high-level data-flow language and execution framework built on MapReduce. It simplifies parallel data processing with a procedural language called Pig Latin.\",\n        \"explanation\": \"Pig provides a simpler alternative to writing Java MapReduce code for complex data transformations.\"\n\n      },\n     {\n        \"topic\": \"Pig Data Model\",\n        \"difficulty\": \"medium\",\n        \"question\": \"Name two complex data types supported by Pig.\",\n        \"answer\": \"Bag, tuple, and map.\",\n        \"explanation\": \"These data types enable Pig to handle nested and semi-structured data effectively.\"\n\n      },\n            {\n        \"topic\": \"Pig vs MapReduce\",\n        \"difficulty\": \"easy\",\n        \"question\": \"How does Pig simplify development compared to MapReduce?\",\n        \"answer\": \"Pig provides a higher-level abstraction than MapReduce, using Pig Latin, which is easier to learn and use than Java for many data processing tasks.\",\n        \"explanation\": \"Pig hides the complexities of MapReduce, allowing developers to focus on data transformations rather than low-level details.\"\n      },\n{\n        \"topic\": \"Pig vs Hive\",\n        \"difficulty\": \"medium\",\n        \"question\": \"What's a key difference in the programming paradigms of Pig and Hive?\",\n        \"answer\": \"Pig is procedural, while Hive is declarative (SQL-like).\",\n        \"explanation\": \"This difference impacts how data transformations are expressed and executed in each tool.\"\n\n      },\n{\n        \"topic\": \"HiveQL\",\n        \"difficulty\": \"easy\",\n        \"question\": \"What type of language is HiveQL?\",\n        \"answer\": \"SQL-like query language for Hive.\",\n        \"explanation\": \"HiveQL allows users familiar with SQL to query data stored in Hadoop.\"\n      },      \n{\n        \"topic\": \"Pig Latin\",\n        \"difficulty\": \"easy\",\n        \"question\": \"What is Pig Latin?\",\n        \"answer\": \"A procedural language used to define data flows in Pig.\",\n        \"explanation\": \"Pig Latin scripts are executed on Pig to perform data transformations.\"\n\n      },\n{\n        \"topic\": \"Pig Commands\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Name two common commands used in Pig Latin.\",\n        \"answer\": \"LOAD, STORE, DUMP, FOREACH, FILTER\",\n        \"explanation\": \"These commands are used for data loading, storing, displaying and basic transformations.\"\n\n      }\n    ]\n  },\n\n\n\n\n\n\n{\n    \"title\": \"Test Plan 3: Big Data Architecture and Comparisons\",\n    \"description\": \"Focuses on the overall Big Data architecture and comparisons between different data processing tools.\",\n    \"time_estimate\": \"40 minutes\",\n    \"questions\": [\n      {\n        \"topic\": \"Big Data Architecture Layers\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Name the five layers of a typical Big Data architecture based on Hadoop.\",\n        \"answer\": \"Data Storage, Data Processing, Data Consumption, Application Support, Application Layer\",\n        \"explanation\": \"These layers represent the different components and functionalities within a Big Data system.\"\n      },\n      {\n        \"topic\": \"Data Storage Layer\",\n        \"difficulty\": \"easy\",\n        \"question\": \"What is the primary function of HDFS in a Big Data architecture?\",\n        \"answer\": \"HDFS (Hadoop Distributed File System) is responsible for storing large datasets across a cluster of machines.\",\n        \"explanation\": \"HDFS provides a distributed, fault-tolerant storage solution for Big Data.\"\n\n      },\n       {\n        \"topic\": \"Data Consumption Layer\",\n        \"difficulty\": \"medium\",\n        \"question\": \"Give examples of activities performed in the Data Consumption Layer.\",\n        \"answer\": \"ETL, analytics, business intelligence, visualization, statistical analysis, machine learning\",\n        \"explanation\": \"This layer utilizes the processed data for various analytical and reporting purposes.\"\n\n      },\n {\n        \"topic\": \"Application Support Layer\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Which layer translates operations into MapReduce or Spark tasks within the Hadoop ecosystem?\",\n        \"answer\": \"Application Support Layer\",\n        \"explanation\": \"Tools in this layer, like Hive and Pig, provide higher-level interfaces that are eventually translated into lower-level MapReduce or Spark tasks for execution.\"\n\n      },\n {\n        \"topic\": \"Pig vs Hive - Data Types\",\n        \"difficulty\": \"medium\",\n        \"question\": \"Which tool, Pig or Hive, offers more flexibility in handling semi-structured data and why?\",\n        \"answer\": \"Pig, because it supports nested data types like bags, tuples, and maps which are well-suited for semi-structured data.\",\n        \"explanation\": \"Hive's relational schema is less flexible when dealing with data that doesn't conform to a strict table structure.\"\n\n\n      },\n{\n        \"topic\": \"Hive vs RDBMS - Transactions\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Why is Hive not suitable for transactional operations like frequent updates and deletes?\",\n        \"answer\": \"Hive is optimized for batch processing and analytical queries on large datasets.  It lacks the fine-grained control and indexing mechanisms necessary for efficient transactional operations.\",\n        \"explanation\": \"RDBMS systems are designed for transactional workloads, whereas Hive is designed for analytical queries on large datasets.\"\n\n\n      },\n{\n        \"topic\": \"Pig vs MapReduce - Complexity\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Which paradigm, Pig or MapReduce, is generally considered simpler for developers and why?\",\n        \"answer\": \"Pig, because it uses a higher-level language (Pig Latin) which abstracts away the complexities of writing Java MapReduce code.\",\n        \"explanation\": \"Pig simplifies development by providing a more concise and user-friendly way to express data transformations.\"\n\n\n      },\n{\n        \"topic\": \"Hive vs RDBMS - Scalability\",\n        \"difficulty\": \"easy\",\n        \"question\": \"Which system, Hive or a typical RDBMS, is better suited for handling massive datasets and why?\",\n        \"answer\": \"Hive.  Hive is designed to scale horizontally across a cluster of machines, enabling it to process petabytes of data. Traditional RDBMS systems struggle to scale to this level.\",\n        \"explanation\": \"Hive leverages the distributed processing capabilities of Hadoop to handle massive datasets.\"\n\n\n      },\n{\n        \"topic\": \"MapReduce Components - Data Locality\",\n        \"difficulty\": \"hard\",\n        \"question\": \"Why is data locality a crucial performance consideration in MapReduce?\",\n        \"answer\": \"Moving computation to the data is more efficient than moving the data to the computation.  MapReduce attempts to schedule map tasks on nodes where the input data resides, minimizing data transfer over the network.\",\n        \"explanation\": \"Network I/O is often a bottleneck in distributed systems, so minimizing data movement is crucial for performance.\"\n\n      },\n{\n        \"topic\": \"Hive - Data Definition\",\n        \"difficulty\": \"easy\",\n        \"question\": \"What is the purpose of DDL (Data Definition Language) in Hive?\",\n        \"answer\": \"DDL is used to define the structure of data in Hive, like creating tables, defining schemas, and managing partitions.\",\n        \"explanation\": \"DDL commands are essential for setting up the data model for Hive queries.\"\n\n      }\n    ]\n  }\n]\n```"
  }
]